# åŠ¨æ€å­˜å‚¨åç«¯ç®¡ç†ä¸æ•…éšœè½¬ç§»

## 1. éœ€æ±‚åˆ†æ

### 1.1 åœºæ™¯æè¿°
- **10ä¸ªèŠ‚ç‚¹é›†ç¾¤**ï¼Œæ¯ä¸ªèŠ‚ç‚¹é…ç½®å¤šç§å­˜å‚¨åç«¯ï¼ˆNVMe-oFã€NFSã€æœ¬åœ°å­˜å‚¨ï¼‰
- **åŠ¨æ€é€‰æ‹©3ä¸ªæ´»è·ƒåç«¯**ï¼Œç”¨äºå½“å‰æ•°æ®å­˜å‚¨
- **æ•…éšœè‡ªåŠ¨è½¬ç§»**ï¼Œå½“æ´»è·ƒåç«¯æ•…éšœæ—¶è‡ªåŠ¨æ›¿æ¢ä¸ºå¥åº·åç«¯
- **é›¶åœæœºå˜æ›´**ï¼Œåç«¯åˆ‡æ¢è¿‡ç¨‹ä¸å½±å“ä¸šåŠ¡è¿ç»­æ€§

### 1.2 æŠ€æœ¯æŒ‘æˆ˜
- **åç«¯çŠ¶æ€ç®¡ç†**ï¼šå®æ—¶ç›‘æ§æ‰€æœ‰å€™é€‰åç«¯çš„å¥åº·çŠ¶æ€
- **åŠ¨æ€é€‰æ‹©ç®—æ³•**ï¼šåŸºäºæ€§èƒ½ã€å¯ç”¨æ€§ã€è´Ÿè½½ç­‰æŒ‡æ ‡æ™ºèƒ½é€‰æ‹©
- **æ•…éšœæ£€æµ‹é€Ÿåº¦**ï¼šå¿«é€Ÿæ£€æµ‹åç«¯æ•…éšœï¼ˆç§’çº§ï¼‰
- **æ•°æ®ä¸€è‡´æ€§**ï¼šåˆ‡æ¢è¿‡ç¨‹ä¸­ä¿è¯æ•°æ®å®Œæ•´æ€§
- **é…ç½®çƒ­æ›´æ–°**ï¼šè¿è¡Œæ—¶åŠ¨æ€æ·»åŠ /ç§»é™¤å€™é€‰åç«¯

## 2. æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„

```c
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Raft Storage Layer                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Dynamic Backend Manager                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚               Active Backend Pool                      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ â”‚
â”‚  â”‚  â”‚   Backend   â”‚ â”‚   Backend   â”‚ â”‚   Backend   â”‚      â”‚ â”‚
â”‚  â”‚  â”‚      A      â”‚ â”‚      B      â”‚ â”‚      C      â”‚      â”‚ â”‚
â”‚  â”‚  â”‚   (Primary) â”‚ â”‚ (Secondary) â”‚ â”‚  (Tertiary) â”‚      â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Candidate Backend Pool                    â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚  â”‚Node-1 â”‚ â”‚Node-2 â”‚ â”‚Node-3 â”‚ â”‚ ... â”‚ â”‚Node-10â”‚    â”‚ â”‚
â”‚  â”‚  â”‚NVMe-oFâ”‚ â”‚  NFS  â”‚ â”‚ Local â”‚ â”‚     â”‚ â”‚ Mixed â”‚    â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                Management Components                   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ â”‚
â”‚  â”‚  â”‚   Health    â”‚ â”‚ Selection   â”‚ â”‚  Failover   â”‚      â”‚ â”‚
â”‚  â”‚  â”‚  Monitor    â”‚ â”‚  Algorithm  â”‚ â”‚  Manager    â”‚      â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒæ•°æ®ç»“æ„

```c
// åç«¯èŠ‚ç‚¹ä¿¡æ¯
typedef struct backend_node {
    char node_id[64];              // èŠ‚ç‚¹ID
    char node_address[256];        // èŠ‚ç‚¹åœ°å€
    
    // åç«¯åˆ—è¡¨
    struct backend_endpoint {
        char endpoint_id[64];      // åç«¯ID
        storage_backend_type_t type; // åç«¯ç±»å‹
        char connection_string[512]; // è¿æ¥å­—ç¬¦ä¸²
        
        // æ€§èƒ½æŒ‡æ ‡
        struct {
            uint64_t avg_latency_us;    // å¹³å‡å»¶è¿Ÿ
            uint64_t bandwidth_mbps;    // å¸¦å®½
            uint64_t iops;              // IOPS
            double error_rate;          // é”™è¯¯ç‡
            uint64_t last_update_time;  // æœ€åæ›´æ–°æ—¶é—´
        } metrics;
        
        // çŠ¶æ€ä¿¡æ¯
        backend_health_state_t health_state; // å¥åº·çŠ¶æ€
        uint64_t last_health_check;   // æœ€åå¥åº·æ£€æŸ¥æ—¶é—´
        uint32_t consecutive_failures; // è¿ç»­å¤±è´¥æ¬¡æ•°
        
        storage_interface_t *interface; // å­˜å‚¨æ¥å£å®ä¾‹
        
        struct backend_endpoint *next;
    } *endpoints;
    
    uint32_t endpoint_count;
    struct backend_node *next;
} backend_node_t;

// åç«¯å¥åº·çŠ¶æ€
typedef enum {
    BACKEND_HEALTHY = 1,           // å¥åº·
    BACKEND_DEGRADED = 2,          // æ€§èƒ½ä¸‹é™
    BACKEND_UNHEALTHY = 3,         // ä¸å¥åº·
    BACKEND_FAILED = 4,            // å®Œå…¨å¤±è´¥
    BACKEND_UNKNOWN = 5            // çŠ¶æ€æœªçŸ¥
} backend_health_state_t;

// æ´»è·ƒåç«¯é…ç½®
typedef struct {
    backend_endpoint_t *endpoints[MAX_ACTIVE_BACKENDS]; // æ´»è·ƒåç«¯åˆ—è¡¨
    uint32_t active_count;         // å½“å‰æ´»è·ƒæ•°é‡
    uint32_t target_count;         // ç›®æ ‡æ´»è·ƒæ•°é‡
    
    // é€‰æ‹©ç­–ç•¥é…ç½®
    struct {
        backend_selection_policy_t policy; // é€‰æ‹©ç­–ç•¥
        double weight_latency;      // å»¶è¿Ÿæƒé‡
        double weight_bandwidth;    // å¸¦å®½æƒé‡
        double weight_reliability;  // å¯é æ€§æƒé‡
        double weight_load;         // è´Ÿè½½æƒé‡
    } selection_config;
    
    // æ•…éšœè½¬ç§»é…ç½®
    struct {
        uint32_t health_check_interval_ms; // å¥åº·æ£€æŸ¥é—´éš”
        uint32_t failure_threshold;        // æ•…éšœé˜ˆå€¼
        uint32_t recovery_threshold;       // æ¢å¤é˜ˆå€¼
        uint32_t failover_timeout_ms;      // æ•…éšœè½¬ç§»è¶…æ—¶
        bool enable_auto_failover;         // å¯ç”¨è‡ªåŠ¨æ•…éšœè½¬ç§»
    } failover_config;
    
    pthread_mutex_t config_lock;   // é…ç½®é”
} active_backend_config_t;

// åŠ¨æ€åç«¯ç®¡ç†å™¨
typedef struct {
    // å€™é€‰åç«¯æ± 
    backend_node_t *candidate_nodes;   // å€™é€‰èŠ‚ç‚¹åˆ—è¡¨
    uint32_t total_candidates;         // æ€»å€™é€‰æ•°é‡
    
    // æ´»è·ƒåç«¯æ± 
    active_backend_config_t active_config;
    
    // å¥åº·ç›‘æ§
    struct {
        pthread_t monitor_thread;     // ç›‘æ§çº¿ç¨‹
        bool monitor_running;         // ç›‘æ§è¿è¡ŒçŠ¶æ€
        uint32_t check_interval_ms;   // æ£€æŸ¥é—´éš”
        
        // ç›‘æ§ç»Ÿè®¡
        uint64_t total_checks;        // æ€»æ£€æŸ¥æ¬¡æ•°
        uint64_t failed_checks;       // å¤±è´¥æ£€æŸ¥æ¬¡æ•°
        uint64_t last_full_scan_time; // æœ€åå…¨é‡æ‰«ææ—¶é—´
    } health_monitor;
    
    // æ•…éšœè½¬ç§»ç®¡ç†
    struct {
        pthread_t failover_thread;    // æ•…éšœè½¬ç§»çº¿ç¨‹
        bool failover_running;        // æ•…éšœè½¬ç§»è¿è¡ŒçŠ¶æ€
        
        // æ•…éšœè½¬ç§»é˜Ÿåˆ—
        struct failover_task {
            backend_endpoint_t *failed_backend;  // å¤±è´¥çš„åç«¯
            backend_endpoint_t *replacement;     // æ›¿æ¢åç«¯
            uint64_t timestamp;                  // æ—¶é—´æˆ³
            struct failover_task *next;
        } *failover_queue;
        
        pthread_mutex_t queue_lock;   // é˜Ÿåˆ—é”
        pthread_cond_t queue_cond;    // é˜Ÿåˆ—æ¡ä»¶å˜é‡
    } failover_mgr;
    
    // æ€§èƒ½ç»Ÿè®¡
    struct {
        uint64_t successful_operations; // æˆåŠŸæ“ä½œæ•°
        uint64_t failed_operations;     // å¤±è´¥æ“ä½œæ•°
        uint64_t failover_count;        // æ•…éšœè½¬ç§»æ¬¡æ•°
        uint64_t backend_switches;      // åç«¯åˆ‡æ¢æ¬¡æ•°
        
        // å»¶è¿Ÿç»Ÿè®¡
        uint64_t avg_operation_latency_us;
        uint64_t p99_operation_latency_us;
    } stats;
    
    pthread_mutex_t manager_lock;     // ç®¡ç†å™¨é”
} dynamic_backend_manager_t;

// åç«¯é€‰æ‹©ç­–ç•¥
typedef enum {
    SELECTION_ROUND_ROBIN = 1,     // è½®è¯¢
    SELECTION_LATENCY_FIRST = 2,   // å»¶è¿Ÿä¼˜å…ˆ
    SELECTION_BANDWIDTH_FIRST = 3, // å¸¦å®½ä¼˜å…ˆ
    SELECTION_LOAD_BALANCED = 4,   // è´Ÿè½½å‡è¡¡
    SELECTION_WEIGHTED_SCORE = 5   // åŠ æƒè¯„åˆ†
} backend_selection_policy_t;
```

## 3. å¥åº·ç›‘æ§ç³»ç»Ÿ

### 3.1 å¤šå±‚æ¬¡å¥åº·æ£€æŸ¥

```c
// å¥åº·æ£€æŸ¥ç±»å‹
typedef enum {
    HEALTH_CHECK_BASIC = 1,        // åŸºç¡€è¿é€šæ€§æ£€æŸ¥
    HEALTH_CHECK_PERFORMANCE = 2,  // æ€§èƒ½æ£€æŸ¥
    HEALTH_CHECK_STRESS = 3,       // å‹åŠ›æµ‹è¯•
    HEALTH_CHECK_FULL = 4          // å…¨é¢æ£€æŸ¥
} health_check_type_t;

// å¥åº·æ£€æŸ¥å®ç°
static int perform_health_check(backend_endpoint_t *endpoint, 
                               health_check_type_t check_type) {
    uint64_t start_time = get_monotonic_time_us();
    int result = 0;
    
    switch (check_type) {
        case HEALTH_CHECK_BASIC:
            result = basic_connectivity_check(endpoint);
            break;
            
        case HEALTH_CHECK_PERFORMANCE:
            result = performance_benchmark_check(endpoint);
            break;
            
        case HEALTH_CHECK_STRESS:
            result = stress_test_check(endpoint);
            break;
            
        case HEALTH_CHECK_FULL:
            result = comprehensive_health_check(endpoint);
            break;
    }
    
    uint64_t end_time = get_monotonic_time_us();
    uint64_t latency = end_time - start_time;
    
    // æ›´æ–°æ€§èƒ½æŒ‡æ ‡
    update_endpoint_metrics(endpoint, latency, result == 0);
    
    return result;
}

// åŸºç¡€è¿é€šæ€§æ£€æŸ¥
static int basic_connectivity_check(backend_endpoint_t *endpoint) {
    if (!endpoint->interface) {
        return -1;
    }
    
    // æ‰§è¡Œè½»é‡çº§è¯»å†™æµ‹è¯•
    char test_data[] = "health_check_probe";
    char read_buffer[32];
    
    // å†™å…¥æµ‹è¯•æ•°æ®
    int write_result = endpoint->interface->write_sync(
        endpoint->interface,
        "_health_check_", 0,
        test_data, sizeof(test_data));
    
    if (write_result != 0) {
        return -1;
    }
    
    // è¯»å–æµ‹è¯•æ•°æ®
    int read_result = endpoint->interface->read_sync(
        endpoint->interface,
        "_health_check_", 0,
        read_buffer, sizeof(test_data));
    
    if (read_result != 0) {
        return -1;
    }
    
    // éªŒè¯æ•°æ®ä¸€è‡´æ€§
    if (memcmp(test_data, read_buffer, sizeof(test_data)) != 0) {
        return -1;
    }
    
    // æ¸…ç†æµ‹è¯•æ•°æ®
    endpoint->interface->delete_file(endpoint->interface, "_health_check_");
    
    return 0;
}

// æ€§èƒ½åŸºå‡†æ£€æŸ¥
static int performance_benchmark_check(backend_endpoint_t *endpoint) {
    const size_t test_size = 4096;      // 4KBæµ‹è¯•å—
    const uint32_t test_count = 100;    // 100æ¬¡æµ‹è¯•
    
    char *test_data = malloc(test_size);
    if (!test_data) return -1;
    
    memset(test_data, 0xAA, test_size);
    
    uint64_t total_latency = 0;
    uint32_t successful_ops = 0;
    
    for (uint32_t i = 0; i < test_count; i++) {
        uint64_t start = get_monotonic_time_us();
        
        char test_path[64];
        snprintf(test_path, sizeof(test_path), "_perf_test_%u", i);
        
        int result = endpoint->interface->write_sync(
            endpoint->interface, test_path, 0, test_data, test_size);
        
        uint64_t end = get_monotonic_time_us();
        
        if (result == 0) {
            total_latency += (end - start);
            successful_ops++;
        }
        
        // æ¸…ç†æµ‹è¯•æ–‡ä»¶
        endpoint->interface->delete_file(endpoint->interface, test_path);
    }
    
    free(test_data);
    
    if (successful_ops == 0) {
        return -1;
    }
    
    // æ›´æ–°æ€§èƒ½æŒ‡æ ‡
    uint64_t avg_latency = total_latency / successful_ops;
    endpoint->metrics.avg_latency_us = avg_latency;
    endpoint->metrics.error_rate = 
        (double)(test_count - successful_ops) / test_count;
    
    // æ€§èƒ½é˜ˆå€¼æ£€æŸ¥
    if (avg_latency > MAX_ACCEPTABLE_LATENCY_US) {
        return -1; // å»¶è¿Ÿè¿‡é«˜
    }
    
    if (endpoint->metrics.error_rate > MAX_ACCEPTABLE_ERROR_RATE) {
        return -1; // é”™è¯¯ç‡è¿‡é«˜
    }
    
    return 0;
}

// å¥åº·ç›‘æ§ä¸»çº¿ç¨‹
static void* health_monitor_thread(void *arg) {
    dynamic_backend_manager_t *manager = (dynamic_backend_manager_t*)arg;
    
    while (manager->health_monitor.monitor_running) {
        uint64_t scan_start = get_monotonic_time_us();
        
        // æ‰«ææ‰€æœ‰å€™é€‰åç«¯
        for (backend_node_t *node = manager->candidate_nodes; 
             node; node = node->next) {
            
            for (backend_endpoint_t *endpoint = node->endpoints;
                 endpoint; endpoint = endpoint->next) {
                
                // æ‰§è¡Œå¥åº·æ£€æŸ¥
                health_check_type_t check_type = determine_check_type(endpoint);
                int check_result = perform_health_check(endpoint, check_type);
                
                // æ›´æ–°å¥åº·çŠ¶æ€
                update_endpoint_health_state(endpoint, check_result);
                
                manager->health_monitor.total_checks++;
                if (check_result != 0) {
                    manager->health_monitor.failed_checks++;
                }
            }
        }
        
        uint64_t scan_end = get_monotonic_time_us();
        manager->health_monitor.last_full_scan_time = scan_end - scan_start;
        
        // ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
        usleep(manager->health_monitor.check_interval_ms * 1000);
    }
    
    return NULL;
}

// åŠ¨æ€å¥åº·çŠ¶æ€æ›´æ–°
static void update_endpoint_health_state(backend_endpoint_t *endpoint, 
                                        int check_result) {
    uint64_t current_time = get_monotonic_time_us();
    endpoint->last_health_check = current_time;
    
    if (check_result == 0) {
        // æ£€æŸ¥æˆåŠŸ
        endpoint->consecutive_failures = 0;
        
        if (endpoint->health_state == BACKEND_FAILED ||
            endpoint->health_state == BACKEND_UNHEALTHY) {
            endpoint->health_state = BACKEND_DEGRADED;
            log_info("Backend %s recovering from failure", endpoint->endpoint_id);
        } else if (endpoint->health_state == BACKEND_DEGRADED) {
            endpoint->health_state = BACKEND_HEALTHY;
            log_info("Backend %s fully recovered", endpoint->endpoint_id);
        }
    } else {
        // æ£€æŸ¥å¤±è´¥
        endpoint->consecutive_failures++;
        
        if (endpoint->consecutive_failures >= FAILURE_THRESHOLD) {
            backend_health_state_t old_state = endpoint->health_state;
            endpoint->health_state = BACKEND_FAILED;
            
            if (old_state != BACKEND_FAILED) {
                log_warning("Backend %s marked as FAILED after %u consecutive failures",
                           endpoint->endpoint_id, endpoint->consecutive_failures);
                
                // è§¦å‘æ•…éšœè½¬ç§»
                trigger_failover_if_active(endpoint);
            }
        } else if (endpoint->consecutive_failures >= DEGRADED_THRESHOLD) {
            if (endpoint->health_state == BACKEND_HEALTHY) {
                endpoint->health_state = BACKEND_DEGRADED;
                log_warning("Backend %s degraded", endpoint->endpoint_id);
            }
        }
    }
}
```

## 4. æ™ºèƒ½åç«¯é€‰æ‹©ç®—æ³•

### 4.1 å¤šå› å­è¯„åˆ†ç®—æ³•

```c
// åç«¯è¯„åˆ†ç»“æ„
typedef struct {
    backend_endpoint_t *endpoint;
    double total_score;          // æ€»è¯„åˆ†
    double latency_score;        // å»¶è¿Ÿè¯„åˆ†
    double bandwidth_score;      // å¸¦å®½è¯„åˆ†
    double reliability_score;    // å¯é æ€§è¯„åˆ†
    double load_score;          // è´Ÿè½½è¯„åˆ†
} backend_score_t;

// æ™ºèƒ½åç«¯é€‰æ‹©
static int select_optimal_backends(dynamic_backend_manager_t *manager,
                                  backend_endpoint_t **selected,
                                  uint32_t target_count) {
    // æ”¶é›†æ‰€æœ‰å¥åº·çš„å€™é€‰åç«¯
    backend_endpoint_t **candidates = NULL;
    uint32_t candidate_count = 0;
    
    collect_healthy_candidates(manager, &candidates, &candidate_count);
    
    if (candidate_count < target_count) {
        log_warning("Insufficient healthy backends: %u available, %u required",
                   candidate_count, target_count);
        return -ENOENT;
    }
    
    // è®¡ç®—è¯„åˆ†
    backend_score_t *scores = malloc(candidate_count * sizeof(backend_score_t));
    if (!scores) {
        free(candidates);
        return -ENOMEM;
    }
    
    for (uint32_t i = 0; i < candidate_count; i++) {
        calculate_backend_score(&scores[i], candidates[i], 
                               &manager->active_config.selection_config);
    }
    
    // æ’åºé€‰æ‹©æœ€ä¼˜çš„åç«¯
    qsort(scores, candidate_count, sizeof(backend_score_t), compare_scores);
    
    // é€‰æ‹©å‰Nä¸ªæœ€ä¼˜åç«¯
    for (uint32_t i = 0; i < target_count; i++) {
        selected[i] = scores[i].endpoint;
        log_info("Selected backend %s with score %.3f",
                selected[i]->endpoint_id, scores[i].total_score);
    }
    
    free(scores);
    free(candidates);
    return 0;
}

// è®¡ç®—åç«¯ç»¼åˆè¯„åˆ†
static void calculate_backend_score(backend_score_t *score,
                                   backend_endpoint_t *endpoint,
                                   const selection_config_t *config) {
    score->endpoint = endpoint;
    
    // å»¶è¿Ÿè¯„åˆ† (è¶Šä½è¶Šå¥½)
    score->latency_score = calculate_latency_score(endpoint->metrics.avg_latency_us);
    
    // å¸¦å®½è¯„åˆ† (è¶Šé«˜è¶Šå¥½)
    score->bandwidth_score = calculate_bandwidth_score(endpoint->metrics.bandwidth_mbps);
    
    // å¯é æ€§è¯„åˆ† (åŸºäºé”™è¯¯ç‡å’Œå†å²æ•…éšœ)
    score->reliability_score = calculate_reliability_score(endpoint);
    
    // è´Ÿè½½è¯„åˆ† (å½“å‰è´Ÿè½½è¶Šä½è¶Šå¥½)
    score->load_score = calculate_load_score(endpoint);
    
    // åŠ æƒæ€»è¯„åˆ†
    score->total_score = 
        config->weight_latency * score->latency_score +
        config->weight_bandwidth * score->bandwidth_score +
        config->weight_reliability * score->reliability_score +
        config->weight_load * score->load_score;
    
    log_debug("Backend %s scores: L=%.3f B=%.3f R=%.3f L=%.3f Total=%.3f",
             endpoint->endpoint_id,
             score->latency_score, score->bandwidth_score,
             score->reliability_score, score->load_score,
             score->total_score);
}

// å»¶è¿Ÿè¯„åˆ†ç®—æ³•
static double calculate_latency_score(uint64_t latency_us) {
    // ä½¿ç”¨åå‘å¯¹æ•°è¯„åˆ†ï¼Œå»¶è¿Ÿè¶Šä½åˆ†æ•°è¶Šé«˜
    const uint64_t baseline_latency = 100;  // 100usåŸºå‡†
    const uint64_t max_latency = 10000;     // 10msæœ€å¤§å¯æ¥å—å»¶è¿Ÿ
    
    if (latency_us <= baseline_latency) {
        return 1.0;  // æ»¡åˆ†
    }
    
    if (latency_us >= max_latency) {
        return 0.1;  // æœ€ä½åˆ†
    }
    
    // å¯¹æ•°è¡°å‡
    double ratio = (double)latency_us / baseline_latency;
    return 1.0 / (1.0 + log(ratio));
}

// å¸¦å®½è¯„åˆ†ç®—æ³•
static double calculate_bandwidth_score(uint64_t bandwidth_mbps) {
    // ä½¿ç”¨å¯¹æ•°å¢é•¿è¯„åˆ†ï¼Œå¸¦å®½è¶Šé«˜åˆ†æ•°è¶Šé«˜
    const uint64_t baseline_bandwidth = 100;  // 100MbpsåŸºå‡†
    const uint64_t max_bandwidth = 10000;     // 10Gbpsä¸Šé™
    
    if (bandwidth_mbps >= max_bandwidth) {
        return 1.0;  // æ»¡åˆ†
    }
    
    if (bandwidth_mbps <= baseline_bandwidth) {
        return 0.3;  // åŸºç¡€åˆ†
    }
    
    // å¯¹æ•°å¢é•¿
    double ratio = (double)bandwidth_mbps / baseline_bandwidth;
    return 0.3 + 0.7 * log(ratio) / log((double)max_bandwidth / baseline_bandwidth);
}

// å¯é æ€§è¯„åˆ†ç®—æ³•
static double calculate_reliability_score(backend_endpoint_t *endpoint) {
    double score = 1.0;
    
    // é”™è¯¯ç‡æƒ©ç½š
    if (endpoint->metrics.error_rate > 0) {
        score *= (1.0 - endpoint->metrics.error_rate);
    }
    
    // è¿ç»­å¤±è´¥æƒ©ç½š
    if (endpoint->consecutive_failures > 0) {
        score *= exp(-0.1 * endpoint->consecutive_failures);
    }
    
    // å¥åº·çŠ¶æ€è°ƒæ•´
    switch (endpoint->health_state) {
        case BACKEND_HEALTHY:
            break;  // æ— è°ƒæ•´
        case BACKEND_DEGRADED:
            score *= 0.8;
            break;
        case BACKEND_UNHEALTHY:
            score *= 0.5;
            break;
        case BACKEND_FAILED:
            score = 0.0;
            break;
        default:
            score *= 0.6;
            break;
    }
    
    return max(score, 0.0);
}

// åŠ¨æ€é‡æ–°é€‰æ‹©è§¦å‘å™¨
static void trigger_backend_reselection(dynamic_backend_manager_t *manager,
                                       const char *reason) {
    log_info("Triggering backend reselection: %s", reason);
    
    pthread_mutex_lock(&manager->manager_lock);
    
    backend_endpoint_t *new_selection[MAX_ACTIVE_BACKENDS];
    uint32_t target_count = manager->active_config.target_count;
    
    int result = select_optimal_backends(manager, new_selection, target_count);
    
    if (result == 0) {
        // æ‰§è¡Œå¹³æ»‘åˆ‡æ¢
        smooth_backend_transition(manager, new_selection, target_count);
        manager->stats.backend_switches++;
    } else {
        log_error("Failed to reselect backends: %d", result);
    }
    
    pthread_mutex_unlock(&manager->manager_lock);
}
```

## 5. é›¶åœæœºæ•…éšœè½¬ç§»

### 5.1 å¹³æ»‘åç«¯åˆ‡æ¢

```c
// å¹³æ»‘åç«¯è¿‡æ¸¡
static int smooth_backend_transition(dynamic_backend_manager_t *manager,
                                   backend_endpoint_t **new_backends,
                                   uint32_t new_count) {
    active_backend_config_t *config = &manager->active_config;
    
    // åˆ†æå˜æ›´
    struct transition_plan {
        backend_endpoint_t **to_remove;   // éœ€è¦ç§»é™¤çš„åç«¯
        backend_endpoint_t **to_add;      // éœ€è¦æ·»åŠ çš„åç«¯
        backend_endpoint_t **to_keep;     // ä¿æŒä¸å˜çš„åç«¯
        uint32_t remove_count;
        uint32_t add_count;
        uint32_t keep_count;
    } plan;
    
    analyze_transition_changes(config->endpoints, config->active_count,
                              new_backends, new_count, &plan);
    
    log_info("Transition plan: keep=%u, add=%u, remove=%u",
             plan.keep_count, plan.add_count, plan.remove_count);
    
    // ç¬¬ä¸€é˜¶æ®µï¼šæ·»åŠ æ–°åç«¯
    for (uint32_t i = 0; i < plan.add_count; i++) {
        backend_endpoint_t *new_backend = plan.to_add[i];
        
        // é¢„çƒ­æ–°åç«¯
        if (warmup_backend(new_backend) != 0) {
            log_error("Failed to warmup new backend %s", 
                     new_backend->endpoint_id);
            continue;
        }
        
        // æ·»åŠ åˆ°æ´»è·ƒåˆ—è¡¨
        config->endpoints[config->active_count] = new_backend;
        config->active_count++;
        
        log_info("Added new backend %s to active pool", 
                new_backend->endpoint_id);
    }
    
    // ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®è¿ç§»ï¼ˆå¦‚éœ€è¦ï¼‰
    if (plan.remove_count > 0) {
        migrate_data_from_removing_backends(&plan);
    }
    
    // ç¬¬ä¸‰é˜¶æ®µï¼šç§»é™¤æ—§åç«¯
    for (uint32_t i = 0; i < plan.remove_count; i++) {
        backend_endpoint_t *old_backend = plan.to_remove[i];
        
        // ä»æ´»è·ƒåˆ—è¡¨ç§»é™¤
        remove_from_active_list(config, old_backend);
        
        // ä¼˜é›…å…³é—­
        graceful_backend_shutdown(old_backend);
        
        log_info("Removed backend %s from active pool", 
                old_backend->endpoint_id);
    }
    
    // æ›´æ–°é…ç½®
    config->active_count = new_count;
    memcpy(config->endpoints, new_backends, 
           new_count * sizeof(backend_endpoint_t*));
    
    log_info("Backend transition completed successfully");
    
    // æ¸…ç†è®¡åˆ’ç»“æ„
    cleanup_transition_plan(&plan);
    
    return 0;
}

// åç«¯é¢„çƒ­
static int warmup_backend(backend_endpoint_t *backend) {
    log_info("Warming up backend %s", backend->endpoint_id);
    
    // å»ºç«‹è¿æ¥
    if (backend->interface->init) {
        int result = backend->interface->init(backend->interface, NULL);
        if (result != 0) {
            log_error("Failed to initialize backend %s: %d", 
                     backend->endpoint_id, result);
            return result;
        }
    }
    
    // æ‰§è¡Œé¢„çƒ­å†™å…¥
    const char warmup_data[] = "backend_warmup_test";
    int write_result = backend->interface->write_sync(
        backend->interface, "_warmup_test", 0,
        warmup_data, sizeof(warmup_data));
    
    if (write_result != 0) {
        log_error("Backend %s warmup write failed: %d",
                 backend->endpoint_id, write_result);
        return write_result;
    }
    
    // æ‰§è¡Œé¢„çƒ­è¯»å–
    char read_buffer[32];
    int read_result = backend->interface->read_sync(
        backend->interface, "_warmup_test", 0,
        read_buffer, sizeof(warmup_data));
    
    if (read_result != 0) {
        log_error("Backend %s warmup read failed: %d",
                 backend->endpoint_id, read_result);
        return read_result;
    }
    
    // æ¸…ç†æµ‹è¯•æ•°æ®
    backend->interface->delete_file(backend->interface, "_warmup_test");
    
    log_info("Backend %s warmed up successfully", backend->endpoint_id);
    return 0;
}

// æ•…éšœè½¬ç§»ä»»åŠ¡å¤„ç†
static void* failover_worker_thread(void *arg) {
    dynamic_backend_manager_t *manager = (dynamic_backend_manager_t*)arg;
    
    while (manager->failover_mgr.failover_running) {
        pthread_mutex_lock(&manager->failover_mgr.queue_lock);
        
        // ç­‰å¾…æ•…éšœè½¬ç§»ä»»åŠ¡
        while (manager->failover_mgr.failover_queue == NULL &&
               manager->failover_mgr.failover_running) {
            pthread_cond_wait(&manager->failover_mgr.queue_cond,
                             &manager->failover_mgr.queue_lock);
        }
        
        if (!manager->failover_mgr.failover_running) {
            pthread_mutex_unlock(&manager->failover_mgr.queue_lock);
            break;
        }
        
        // å–å‡ºæ•…éšœè½¬ç§»ä»»åŠ¡
        struct failover_task *task = manager->failover_mgr.failover_queue;
        manager->failover_mgr.failover_queue = task->next;
        
        pthread_mutex_unlock(&manager->failover_mgr.queue_lock);
        
        // æ‰§è¡Œæ•…éšœè½¬ç§»
        execute_failover_task(manager, task);
        
        // æ¸…ç†ä»»åŠ¡
        free(task);
        
        manager->stats.failover_count++;
    }
    
    return NULL;
}

// æ‰§è¡Œæ•…éšœè½¬ç§»ä»»åŠ¡
static int execute_failover_task(dynamic_backend_manager_t *manager,
                                struct failover_task *task) {
    uint64_t start_time = get_monotonic_time_us();
    
    log_info("Executing failover: replacing %s with %s",
             task->failed_backend->endpoint_id,
             task->replacement ? task->replacement->endpoint_id : "auto-select");
    
    pthread_mutex_lock(&manager->manager_lock);
    
    active_backend_config_t *config = &manager->active_config;
    
    // æ‰¾åˆ°å¤±è´¥åç«¯åœ¨æ´»è·ƒåˆ—è¡¨ä¸­çš„ä½ç½®
    int failed_index = -1;
    for (uint32_t i = 0; i < config->active_count; i++) {
        if (config->endpoints[i] == task->failed_backend) {
            failed_index = i;
            break;
        }
    }
    
    if (failed_index == -1) {
        log_warning("Failed backend %s not found in active list",
                   task->failed_backend->endpoint_id);
        pthread_mutex_unlock(&manager->manager_lock);
        return -1;
    }
    
    // é€‰æ‹©æ›¿æ¢åç«¯
    backend_endpoint_t *replacement = task->replacement;
    if (!replacement) {
        replacement = select_best_replacement(manager, task->failed_backend);
    }
    
    if (!replacement) {
        log_error("No suitable replacement found for failed backend %s",
                 task->failed_backend->endpoint_id);
        pthread_mutex_unlock(&manager->manager_lock);
        return -1;
    }
    
    // é¢„çƒ­æ›¿æ¢åç«¯
    if (warmup_backend(replacement) != 0) {
        log_error("Failed to warmup replacement backend %s",
                 replacement->endpoint_id);
        pthread_mutex_unlock(&manager->manager_lock);
        return -1;
    }
    
    // åŸå­æ€§æ›¿æ¢
    config->endpoints[failed_index] = replacement;
    
    // ä¼˜é›…å…³é—­å¤±è´¥åç«¯
    graceful_backend_shutdown(task->failed_backend);
    
    uint64_t end_time = get_monotonic_time_us();
    uint64_t failover_latency = end_time - start_time;
    
    log_info("Failover completed in %lu us: %s -> %s",
             failover_latency,
             task->failed_backend->endpoint_id,
             replacement->endpoint_id);
    
    pthread_mutex_unlock(&manager->manager_lock);
    
    return 0;
}
```

## 6. é…ç½®å’Œç®¡ç†æ¥å£

### 6.1 é…ç½®æ–‡ä»¶æ ¼å¼

```yaml
# dynamic_backend_config.yaml
dynamic_backend_manager:
  # åŸºæœ¬é…ç½®
  target_active_backends: 3
  health_check_interval_ms: 5000
  
  # å€™é€‰åç«¯èŠ‚ç‚¹
  candidate_nodes:
    - node_id: "node-01"
      address: "192.168.1.101"
      endpoints:
        - endpoint_id: "nvmeof-01"
          type: "nvmeof"
          connection_string: "nvme://192.168.1.101:4420/namespace1"
          weight: 1.0
        - endpoint_id: "nfs-01"
          type: "nfs"
          connection_string: "nfs://192.168.1.101:/export/xdevice"
          weight: 0.8
    
    - node_id: "node-02"
      address: "192.168.1.102"
      endpoints:
        - endpoint_id: "nvmeof-02"
          type: "nvmeof"
          connection_string: "nvme://192.168.1.102:4420/namespace1"
          weight: 1.0
        - endpoint_id: "local-02"
          type: "local"
          connection_string: "file:///data/xdevice"
          weight: 0.6
    
    # ... æ›´å¤šèŠ‚ç‚¹é…ç½®
    - node_id: "node-10"
      address: "192.168.1.110"
      endpoints:
        - endpoint_id: "mixed-10"
          type: "nvmeof"
          connection_string: "nvme://192.168.1.110:4420/namespace1"
          weight: 0.9

  # é€‰æ‹©ç­–ç•¥é…ç½®
  selection_policy:
    algorithm: "weighted_score"
    weights:
      latency: 0.4      # å»¶è¿Ÿæƒé‡40%
      bandwidth: 0.3    # å¸¦å®½æƒé‡30%
      reliability: 0.2  # å¯é æ€§æƒé‡20%
      load: 0.1        # è´Ÿè½½æƒé‡10%
  
  # æ•…éšœè½¬ç§»é…ç½®
  failover:
    enable_auto_failover: true
    failure_threshold: 3        # è¿ç»­3æ¬¡å¤±è´¥è§¦å‘æ•…éšœè½¬ç§»
    recovery_threshold: 5       # è¿ç»­5æ¬¡æˆåŠŸæ‰è®¤ä¸ºæ¢å¤
    failover_timeout_ms: 30000  # æ•…éšœè½¬ç§»è¶…æ—¶30ç§’
    enable_data_migration: false # ç¦ç”¨æ•°æ®è¿ç§»ï¼ˆWALåœºæ™¯ï¼‰
  
  # ç›‘æ§é…ç½®
  monitoring:
    enable_detailed_metrics: true
    metrics_export_interval_ms: 10000
    health_check_types:
      - "basic"         # åŸºç¡€æ£€æŸ¥
      - "performance"   # æ€§èƒ½æ£€æŸ¥
```

### 6.2 è¿è¡Œæ—¶ç®¡ç†API

```c
// ç®¡ç†APIæ¥å£
typedef struct {
    // é…ç½®ç®¡ç†
    int (*load_config)(dynamic_backend_manager_t *manager, const char *config_file);
    int (*reload_config)(dynamic_backend_manager_t *manager);
    int (*save_config)(dynamic_backend_manager_t *manager, const char *config_file);
    
    // åç«¯ç®¡ç†
    int (*add_candidate_node)(dynamic_backend_manager_t *manager, 
                             const backend_node_t *node);
    int (*remove_candidate_node)(dynamic_backend_manager_t *manager, 
                                const char *node_id);
    int (*update_node_config)(dynamic_backend_manager_t *manager,
                             const char *node_id, const backend_node_t *new_config);
    
    // æ´»è·ƒåç«¯ç®¡ç†
    int (*force_backend_switch)(dynamic_backend_manager_t *manager,
                               const char *old_backend_id, const char *new_backend_id);
    int (*set_backend_weight)(dynamic_backend_manager_t *manager,
                             const char *backend_id, double weight);
    int (*trigger_reselection)(dynamic_backend_manager_t *manager, const char *reason);
    
    // ç›‘æ§å’ŒçŠ¶æ€
    int (*get_active_backends)(dynamic_backend_manager_t *manager,
                              backend_endpoint_t ***backends, uint32_t *count);
    int (*get_backend_status)(dynamic_backend_manager_t *manager,
                             const char *backend_id, backend_status_t *status);
    int (*get_manager_stats)(dynamic_backend_manager_t *manager, 
                            manager_stats_t *stats);
    
} dynamic_backend_api_t;

// CLIå‘½ä»¤å®ç°
int cli_list_candidates(int argc, char **argv) {
    dynamic_backend_manager_t *manager = get_global_backend_manager();
    
    printf("Candidate Backend Nodes:\n");
    printf("%-12s %-15s %-12s %-10s %-12s\n", 
           "NODE_ID", "ADDRESS", "ENDPOINT_ID", "TYPE", "STATUS");
    printf("================================================================\n");
    
    for (backend_node_t *node = manager->candidate_nodes; node; node = node->next) {
        for (backend_endpoint_t *endpoint = node->endpoints; 
             endpoint; endpoint = endpoint->next) {
            
            const char *status_str = health_state_to_string(endpoint->health_state);
            const char *type_str = backend_type_to_string(endpoint->type);
            
            printf("%-12s %-15s %-12s %-10s %-12s\n",
                   node->node_id, node->node_address,
                   endpoint->endpoint_id, type_str, status_str);
        }
    }
    
    return 0;
}

int cli_list_active_backends(int argc, char **argv) {
    dynamic_backend_manager_t *manager = get_global_backend_manager();
    active_backend_config_t *config = &manager->active_config;
    
    printf("Active Backend Pool:\n");
    printf("%-12s %-10s %-12s %-12s %-12s\n",
           "ENDPOINT_ID", "TYPE", "LATENCY_US", "BANDWIDTH", "LOAD");
    printf("================================================================\n");
    
    pthread_mutex_lock(&config->config_lock);
    
    for (uint32_t i = 0; i < config->active_count; i++) {
        backend_endpoint_t *endpoint = config->endpoints[i];
        
        printf("%-12s %-10s %-12lu %-12lu %.2f%%\n",
               endpoint->endpoint_id,
               backend_type_to_string(endpoint->type),
               endpoint->metrics.avg_latency_us,
               endpoint->metrics.bandwidth_mbps,
               get_backend_load_percentage(endpoint));
    }
    
    pthread_mutex_unlock(&config->config_lock);
    
    return 0;
}

int cli_force_failover(int argc, char **argv) {
    if (argc < 3) {
        printf("Usage: %s <failed_backend_id> [replacement_backend_id]\n", argv[0]);
        return -1;
    }
    
    const char *failed_backend_id = argv[1];
    const char *replacement_backend_id = (argc > 2) ? argv[2] : NULL;
    
    dynamic_backend_manager_t *manager = get_global_backend_manager();
    
    // æ‰¾åˆ°å¤±è´¥çš„åç«¯
    backend_endpoint_t *failed_backend = find_backend_by_id(manager, failed_backend_id);
    if (!failed_backend) {
        printf("Error: Backend '%s' not found\n", failed_backend_id);
        return -1;
    }
    
    // æ‰¾åˆ°æ›¿æ¢åç«¯ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    backend_endpoint_t *replacement = NULL;
    if (replacement_backend_id) {
        replacement = find_backend_by_id(manager, replacement_backend_id);
        if (!replacement) {
            printf("Error: Replacement backend '%s' not found\n", replacement_backend_id);
            return -1;
        }
    }
    
    // è§¦å‘æ•…éšœè½¬ç§»
    struct failover_task *task = malloc(sizeof(struct failover_task));
    task->failed_backend = failed_backend;
    task->replacement = replacement;
    task->timestamp = get_monotonic_time_us();
    task->next = NULL;
    
    pthread_mutex_lock(&manager->failover_mgr.queue_lock);
    
    // æ·»åŠ åˆ°æ•…éšœè½¬ç§»é˜Ÿåˆ—
    if (manager->failover_mgr.failover_queue == NULL) {
        manager->failover_mgr.failover_queue = task;
    } else {
        struct failover_task *tail = manager->failover_mgr.failover_queue;
        while (tail->next) tail = tail->next;
        tail->next = task;
    }
    
    pthread_cond_signal(&manager->failover_mgr.queue_cond);
    pthread_mutex_unlock(&manager->failover_mgr.queue_lock);
    
    printf("Failover triggered: %s -> %s\n", 
           failed_backend_id, 
           replacement_backend_id ? replacement_backend_id : "auto-select");
    
    return 0;
}

// é…ç½®çƒ­é‡è½½
int cli_reload_config(int argc, char **argv) {
    const char *config_file = (argc > 1) ? argv[1] : "config/dynamic_backend.yaml";
    
    dynamic_backend_manager_t *manager = get_global_backend_manager();
    
    printf("Reloading configuration from %s...\n", config_file);
    
    int result = manager->api.reload_config(manager);
    if (result == 0) {
        printf("Configuration reloaded successfully\n");
    } else {
        printf("Failed to reload configuration: %d\n", result);
    }
    
    return result;
}
```

## 7. ä½¿ç”¨ç¤ºä¾‹

### 7.1 åˆå§‹åŒ–å’Œé…ç½®

```c
#include "dynamic_backend_manager.h"

int main() {
    // åˆ›å»ºåŠ¨æ€åç«¯ç®¡ç†å™¨
    dynamic_backend_manager_t *manager = create_dynamic_backend_manager();
    if (!manager) {
        fprintf(stderr, "Failed to create backend manager\n");
        return -1;
    }
    
    // åŠ è½½é…ç½®
    if (load_backend_config(manager, "config/dynamic_backend.yaml") != 0) {
        fprintf(stderr, "Failed to load backend configuration\n");
        return -1;
    }
    
    // å¯åŠ¨ç®¡ç†å™¨
    if (start_dynamic_backend_manager(manager) != 0) {
        fprintf(stderr, "Failed to start backend manager\n");
        return -1;
    }
    
    printf("Dynamic backend manager started with %u candidate nodes\n",
           count_candidate_nodes(manager));
    
    // ç­‰å¾…åˆå§‹åç«¯é€‰æ‹©å®Œæˆ
    wait_for_initial_selection(manager, 10000); // 10ç§’è¶…æ—¶
    
    // è·å–å½“å‰æ´»è·ƒåç«¯
    backend_endpoint_t **active_backends = NULL;
    uint32_t active_count = 0;
    
    get_active_backends(manager, &active_backends, &active_count);
    
    printf("Selected %u active backends:\n", active_count);
    for (uint32_t i = 0; i < active_count; i++) {
        printf("  - %s (%s)\n", 
               active_backends[i]->endpoint_id,
               backend_type_to_string(active_backends[i]->type));
    }
    
    // ä½¿ç”¨åç«¯è¿›è¡ŒI/Oæ“ä½œ
    demonstrate_io_operations(active_backends, active_count);
    
    // æ¸…ç†
    stop_dynamic_backend_manager(manager);
    destroy_dynamic_backend_manager(manager);
    
    return 0;
}

// I/Oæ“ä½œç¤ºä¾‹
static void demonstrate_io_operations(backend_endpoint_t **backends, uint32_t count) {
    const char *test_data = "Hello, dynamic backend!";
    char read_buffer[256];
    
    // è½®è¯¢å†™å…¥åˆ°æ‰€æœ‰æ´»è·ƒåç«¯
    for (uint32_t i = 0; i < count; i++) {
        backend_endpoint_t *backend = backends[i];
        
        printf("Writing to backend %s...\n", backend->endpoint_id);
        
        int write_result = backend->interface->write_sync(
            backend->interface, "test_file", 0, test_data, strlen(test_data));
        
        if (write_result == 0) {
            printf("  Write successful\n");
            
            // è¯»å–éªŒè¯
            int read_result = backend->interface->read_sync(
                backend->interface, "test_file", 0, read_buffer, strlen(test_data));
            
            if (read_result == 0) {
                read_buffer[strlen(test_data)] = '\0';
                printf("  Read successful: %s\n", read_buffer);
            } else {
                printf("  Read failed: %d\n", read_result);
            }
        } else {
            printf("  Write failed: %d\n", write_result);
        }
    }
}

// æ•…éšœæ¨¡æ‹Ÿå’Œæ¢å¤
static void simulate_backend_failure(dynamic_backend_manager_t *manager) {
    printf("\n=== Simulating Backend Failure ===\n");
    
    // äººä¸ºæ ‡è®°ä¸€ä¸ªåç«¯ä¸ºå¤±è´¥çŠ¶æ€
    backend_endpoint_t **active_backends = NULL;
    uint32_t active_count = 0;
    
    get_active_backends(manager, &active_backends, &active_count);
    
    if (active_count > 0) {
        backend_endpoint_t *victim = active_backends[0];
        printf("Marking backend %s as failed\n", victim->endpoint_id);
        
        // æ¨¡æ‹Ÿå¤±è´¥
        victim->health_state = BACKEND_FAILED;
        victim->consecutive_failures = 10;
        
        // è§¦å‘æ•…éšœè½¬ç§»
        trigger_failover_if_active(victim);
        
        // ç­‰å¾…æ•…éšœè½¬ç§»å®Œæˆ
        sleep(5);
        
        // æ£€æŸ¥æ–°çš„æ´»è·ƒåç«¯
        get_active_backends(manager, &active_backends, &active_count);
        printf("After failover, active backends:\n");
        for (uint32_t i = 0; i < active_count; i++) {
            printf("  - %s\n", active_backends[i]->endpoint_id);
        }
    }
}
```

## 8. ç›‘æ§å’Œå‘Šè­¦

### 8.1 å…³é”®æŒ‡æ ‡ç›‘æ§

```c
// ç›‘æ§æŒ‡æ ‡å®šä¹‰
typedef struct {
    // å¯ç”¨æ€§æŒ‡æ ‡
    double overall_availability;        // æ•´ä½“å¯ç”¨æ€§
    uint32_t healthy_backend_count;     // å¥åº·åç«¯æ•°é‡
    uint32_t total_backend_count;       // æ€»åç«¯æ•°é‡
    
    // æ€§èƒ½æŒ‡æ ‡
    uint64_t avg_operation_latency_us;  // å¹³å‡æ“ä½œå»¶è¿Ÿ
    uint64_t p99_operation_latency_us;  // P99æ“ä½œå»¶è¿Ÿ
    uint64_t operations_per_second;     // æ¯ç§’æ“ä½œæ•°
    
    // æ•…éšœè½¬ç§»æŒ‡æ ‡
    uint32_t failover_events_last_hour; // æœ€è¿‘1å°æ—¶æ•…éšœè½¬ç§»æ¬¡æ•°
    uint64_t avg_failover_time_ms;      // å¹³å‡æ•…éšœè½¬ç§»æ—¶é—´
    uint32_t failed_failovers;          // å¤±è´¥çš„æ•…éšœè½¬ç§»æ¬¡æ•°
    
    // åç«¯å¥åº·æŒ‡æ ‡
    struct backend_health_summary {
        uint32_t healthy_count;
        uint32_t degraded_count;
        uint32_t unhealthy_count;
        uint32_t failed_count;
    } health_summary;
    
} monitoring_metrics_t;

// é¢„å®šä¹‰å‘Šè­¦è§„åˆ™
static alert_rule_t default_alert_rules[] = {
    {
        .rule_name = "HighFailoverRate",
        .condition = "failover_events_last_hour > 10",
        .severity = ALERT_WARNING,
        .evaluation_interval_sec = 60,
        .consecutive_threshold = 2,
        .enabled = true
    },
    {
        .rule_name = "LowBackendAvailability", 
        .condition = "overall_availability < 0.7",
        .severity = ALERT_CRITICAL,
        .evaluation_interval_sec = 30,
        .consecutive_threshold = 1,
        .enabled = true
    },
    {
        .rule_name = "HighOperationLatency",
        .condition = "p99_operation_latency_us > 10000", // > 10ms
        .severity = ALERT_WARNING,
        .evaluation_interval_sec = 120,
        .consecutive_threshold = 3,
        .enabled = true
    },
    {
        .rule_name = "NoHealthyBackends",
        .condition = "healthy_backend_count == 0",
        .severity = ALERT_CRITICAL,
        .evaluation_interval_sec = 10,
        .consecutive_threshold = 1,
        .enabled = true
    }
};
```

## 9. æ€»ç»“

è¿™ä¸ªåŠ¨æ€å­˜å‚¨åç«¯ç®¡ç†ç³»ç»Ÿæä¾›äº†ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

### ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½**
1. **æ™ºèƒ½åç«¯é€‰æ‹©**ï¼šåŸºäºå»¶è¿Ÿã€å¸¦å®½ã€å¯é æ€§ã€è´Ÿè½½çš„å¤šå› å­è¯„åˆ†ç®—æ³•
2. **é›¶åœæœºæ•…éšœè½¬ç§»**ï¼šç§’çº§æ•…éšœæ£€æµ‹ï¼Œå¹³æ»‘åç«¯åˆ‡æ¢
3. **å®æ—¶å¥åº·ç›‘æ§**ï¼šå¤šå±‚æ¬¡å¥åº·æ£€æŸ¥ï¼Œæ™ºèƒ½çŠ¶æ€ç®¡ç†
4. **é…ç½®çƒ­æ›´æ–°**ï¼šè¿è¡Œæ—¶åŠ¨æ€æ·»åŠ /ç§»é™¤å€™é€‰åç«¯
5. **å…¨é¢ç›‘æ§å‘Šè­¦**ï¼šä¸°å¯Œçš„æ€§èƒ½æŒ‡æ ‡å’Œè‡ªåŠ¨å‘Šè­¦

### ğŸš€ **æŠ€æœ¯ç‰¹ç‚¹**
- **é«˜å¯ç”¨æ€§**ï¼šè‡ªåŠ¨æ•…éšœè½¬ç§»ï¼Œç¡®ä¿æœåŠ¡è¿ç»­æ€§
- **é«˜æ€§èƒ½**ï¼šæ™ºèƒ½è´Ÿè½½å‡è¡¡ï¼Œä¼˜åŒ–I/Oè·¯å¾„
- **æ˜“ç®¡ç†**ï¼šCLIå·¥å…·ï¼Œé…ç½®æ–‡ä»¶é©±åŠ¨
- **å¯è§‚æµ‹æ€§**ï¼šè¯¦ç»†ç›‘æ§æŒ‡æ ‡ï¼Œå®æ—¶çŠ¶æ€å±•ç¤º

### ğŸ“‹ **ä½¿ç”¨åœºæ™¯**
- âœ… 10èŠ‚ç‚¹é›†ç¾¤ï¼Œæ¯èŠ‚ç‚¹å¤šç§å­˜å‚¨åç«¯ï¼ˆNVMe-oFã€NFSã€æœ¬åœ°ï¼‰
- âœ… åŠ¨æ€é€‰æ‹©3ä¸ªæœ€ä¼˜åç«¯ä½œä¸ºæ´»è·ƒå­˜å‚¨
- âœ… è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œæ›¿æ¢
- âœ… é›¶åœæœºè¿ç»´æ“ä½œ

è¿™ä¸ªè®¾è®¡å®Œç¾è§£å†³äº†æ‚¨æå‡ºçš„éœ€æ±‚ï¼Œå®ç°äº†çµæ´»çš„å¤šåç«¯ç®¡ç†å’Œè‡ªåŠ¨æ•…éšœè½¬ç§»èƒ½åŠ›ï¼
